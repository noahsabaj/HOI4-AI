# src/ai/ultimate/train_ultimate.py
"""
Training loop for Ultimate HOI4 AI
Integrates with your existing main.py
"""

import os
import sys
import time
import keyboard
import pyautogui
from PIL import ImageGrab
import torch
import numpy as np
from datetime import datetime
from queue import Queue, Empty, Full
from threading import Thread, Event
import json
import re
from typing import Dict
from src.config import CONFIG
from .hf_strategic_evaluator import HuggingFaceStrategicEvaluator
from src.utils.common import extract_number, detect_screen_type
from src.utils.logger import get_logger
# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))))
# Import our Ultimate AI
from .ultimate_ai import UltimateHOI4AI, ActionSpace
# Import your existing evaluation system
from src.strategy.evaluation import StrategicEvaluator

# ‚îÄ‚îÄ window-focus helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def _ensure_hoi4_window_foreground() -> None:
    """
    Bring the Hearts of Iron IV window to the foreground if it isn‚Äôt.
    Works on Windows; NO-OP on other OSes.
    """
    try:
        import pygetwindow as gw
        win = next(w for w in gw.getWindowsWithTitle('Hearts of Iron') if w.isVisible)
        if not win.isActive:
            win.activate()
    except Exception:
        # don‚Äôt crash if pygetwindow missing or window not found
        pass


class UltimateTrainer:
    """
    Manages the training loop for Ultimate AI
    Works alongside your existing code
    """

    def __init__(self):
        print("""
        ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
        ‚ïë              Ultimate HOI4 AI Training System             ‚ïë
        ‚ïë                                                           ‚ïë
        ‚ïë  DreamerV3 + RND + NEC = Autonomous Learning              ‚ïë
        ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
        """)

        # ‚îÄ‚îÄ colour logger ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        self.log = get_logger("trainer")
        self.log.info("üöÄ Trainer initialized")

        # Initialize Ultimate AI & evaluator
        self.ai = UltimateHOI4AI()
        self.evaluator = StrategicEvaluator()

        # Use the reasoning-optimized Phi-4 mini model
        print("  ü§ó Initializing Phi-4 strategic evaluator...")
        self.strategic_reasoner = HuggingFaceStrategicEvaluator(
            model_name="microsoft/Phi-4-mini-reasoning",  # 3.8B, optimized for reasoning!
            device='cuda' if torch.cuda.is_available() else 'cpu',
            evaluation_frequency=30
        )

        self.learn_queue = Queue(maxsize=64)  # holds (prev, act, next, r)
        self.shutdown_event = Event()
        self.learn_thread = Thread(
            target=self._learner_loop,
            name="learner-thread",
            daemon=True,
        )
        self.learn_thread.start()

        # Training state
        self.is_training = False
        self.session_start = time.time()
        self.last_screenshot = None
        self.last_action = None

        # Metrics
        self.session_metrics = {
            "actions_taken": 0,
            "discoveries": [],
            "key_moments": [],
            "current_strategy": "exploring",
        }

        # Strategic health tracking
        self.last_strategic_health = 0.0

        # Auto-save & status intervals
        self.last_save = time.time()
        self.save_interval = 300  # 5 min, could move to CONFIG later
        self.last_status_update = time.time()
        self.status_interval = 10  # 10 s

    def run(self):
        """Main training loop"""
        self.log.info("üìã Controls:")
        self.log.info("  F5: Start/Resume Training")
        self.log.info("  F6: Pause Training")
        self.log.info("  F7: Save Progress")
        self.log.info("  F8: Show Statistics")
        self.log.info("  ESC (hold 2s): Exit")
        self.log.info("üéÆ Start HOI4 and press F5 when ready...")

        # Control flags
        esc_start_time = None

        while True:
            # Handle controls
            if keyboard.is_pressed('f5') and not self.is_training:
                self.start_training()
                time.sleep(0.3)

            elif keyboard.is_pressed('f6') and self.is_training:
                self.pause_training()
                time.sleep(0.3)

            elif keyboard.is_pressed('f7'):
                self.save_progress()
                time.sleep(0.3)

            elif keyboard.is_pressed('f8'):
                self.show_statistics()
                time.sleep(0.3)

            elif keyboard.is_pressed('escape'):
                if esc_start_time is None:
                    esc_start_time = time.time()
                elif time.time() - esc_start_time > 2.0:
                    print("\nüõë Stopping Ultimate AI...")
                    self.cleanup()
                    break
            else:
                esc_start_time = None

            # Training step
            if self.is_training:
                self.training_step()

            # Auto-save
            if time.time() - self.last_save > self.save_interval:
                self.save_progress()
                self.last_save = time.time()

            # Status update
            if time.time() - self.last_status_update > self.status_interval:
                self.update_status()
                self.last_status_update = time.time()

            # Small delay to prevent CPU overuse
            time.sleep(0.005)

    def training_step(self):
        """Single non-blocking env step with async learning."""
        try:
            t0 = time.perf_counter()

            # 1. Screenshot
            screenshot = ImageGrab.grab()
            t1 = time.perf_counter()

            # 2. Decide
            action = self.ai.act(screenshot)
            t2 = time.perf_counter()

            # 3. Execute
            self.execute_action(action)
            t3 = time.perf_counter()

            # 4. Tiny pause (keep game responsive)
            time.sleep(0.005)
            t4 = time.perf_counter()

            # 5. Queue transition for learner thread
            if self.last_screenshot is not None:
                reward = self.calculate_reward(
                    self.last_screenshot, screenshot, action
                )
                try:
                    self.learn_queue.put_nowait(
                        (self.last_screenshot, self.last_action,
                         screenshot, reward)
                    )
                except Full:
                    # drop transition if queue is saturated
                    self.log.warning("‚ö†Ô∏è learn queue full ‚Äì dropping step")

            # 6. Book-keeping
            self.last_screenshot = screenshot
            self.last_action = action
            self.session_metrics["actions_taken"] += 1

            # 7. Timing log (optional: drop to DEBUG)
            dt = lambda a, b: (b - a) * 1000  # ms
            self.log.info(
                f"‚è± cap {dt(t0,t1):4.0f} ms | decide {dt(t1,t2):4.0f} ms | "
                f"act {dt(t2,t3):4.0f} ms | pause {dt(t3,t4):3.0f} ms | "
                f"queued {self.learn_queue.qsize():3d}"
            )

        except Exception as e:
            import traceback
            self.log.error(f"‚ùå training_step error: {e}")
            traceback.print_exc()
            self.pause_training()

    def _learner_loop(self) -> None:
        """
        Runs in a daemon thread.
        Blocks on `learn_queue` and calls `self.ai.learn_from_transition`.
        """
        while not self.shutdown_event.is_set():
            try:
                prev_scr, prev_act, next_scr, reward = self.learn_queue.get(timeout=0.5)
            except Empty:
                continue  # nothing to do ‚Äì loop again
            try:
                self.ai.learn_from_transition(prev_scr, prev_act, next_scr, reward)
            except Exception as e:
                self.log.error(f"‚ôªÔ∏è learner thread error: {e}")

    def execute_action(self, action: Dict):
        """Execute the AI's chosen action"""
        if action['type'] == 'click':
            pyautogui.click(
                action['x'],
                action['y'],
                button=action.get('button', 'left')
            )
            # Log every 10th action or significant ones
            if (self.session_metrics['actions_taken'] % 10 == 0 or
                    any(keyword in action.get('description', '').lower()
                        for keyword in ['factory', 'build', 'research', 'focus', 'production'])):
                print(
                    f"\nüñ±Ô∏è [{self.session_metrics['actions_taken']}] {action['description']} at ({action['x']}, {action['y']})")

        elif action['type'] == 'key':
            _ensure_hoi4_window_foreground()
            key = action['key']
            pyautogui.keyDown(key)
            time.sleep(0.05)
            pyautogui.keyUp(key)
            print(f"\n‚å®Ô∏è [{self.session_metrics['actions_taken']}] Pressed {key} - {action.get('description', '')}")


        elif action['type'] == 'key':
            _ensure_hoi4_window_foreground()  # ‚Üê NEW
            key = action['key']
            pyautogui.keyDown(key)  # press
            time.sleep(0.05)  # hold 50 ms
            pyautogui.keyUp(key)  # release
            if key in ['q', 'w', 'e', 'r', 't', 'b']:
                self.log.debug(f"‚å®Ô∏è sent {key}")

    def log_game_state(self, ocr_data: Dict, screen_type: str):
        """Log important game state information"""
        # Only log every 20 actions to avoid spam
        if self.session_metrics['actions_taken'] % 20 == 0:
            date = ocr_data.get('date', 'Unknown')
            pp = self.extract_number(ocr_data.get('political_power', '0'))

            print(f"\nüìä Game State Check:")
            print(f"   Date: {date}")
            print(f"   Screen: {screen_type}")
            print(f"   Political Power: {pp}")

            # Log any factory info
            factory_text = ocr_data.get('factories', '')
            if factory_text:
                print(f"   Factories: {factory_text}")

            # Log first few OCR entries to debug
            print(f"   OCR Keys: {list(ocr_data.keys())[:5]}...")

    def calculate_reward(self, prev_screenshot, curr_screenshot, action) -> float:
        """
        Calculate reward for the transition including DeepSeek-R1 evaluation
        """
        # Basic reward structure
        reward = -0.01  # Small negative for time pressure

        # Use your existing OCR to detect changes
        prev_ocr = self.ai.ocr.extract_all_text(prev_screenshot)
        curr_ocr = self.ai.ocr.extract_all_text(curr_screenshot)

        # After OCR extraction
        curr_screen = self.detect_screen_type(curr_ocr)
        self.log_game_state(curr_ocr, curr_screen)

        # Reward for changing screens (exploration)
        prev_screen = self.detect_screen_type(prev_ocr)
        curr_screen = self.detect_screen_type(curr_ocr)
        if prev_screen != curr_screen:
            reward += 2.0

        # Reward for resource changes
        prev_pp = self.extract_number(prev_ocr.get('political_power', '0'))
        curr_pp = self.extract_number(curr_ocr.get('political_power', '0'))

        if curr_pp > prev_pp:
            reward += 0.5  # PP increased

        # Factory changes (very good!)
        prev_factories = prev_ocr.get('factories', '')
        curr_factories = curr_ocr.get('factories', '')

        if 'factory' in action.get('description', '').lower() and curr_factories != prev_factories:
            reward += 5.0  # Likely built a factory

        # Use your strategic evaluator
        game_state = self.build_game_state(curr_ocr)
        strategic_eval = self.evaluator.evaluate_game_state(game_state, curr_ocr)

        # Reward for improving strategic position
        if hasattr(self, 'last_strategic_health'):
            health_delta = strategic_eval['strategic_health'] - self.last_strategic_health
            reward += health_delta * 10

        self.last_strategic_health = strategic_eval['strategic_health']

        # Pixel-difference novelty bonus
        prev_arr = np.asarray(prev_screenshot, dtype=np.float32) / 255.0
        curr_arr = np.asarray(curr_screenshot, dtype=np.float32) / 255.0
        diff_ratio = (np.abs(prev_arr - curr_arr) > 0.05).mean()
        reward += diff_ratio * 2.0

        # === DEEPSEEK-R1 STRATEGIC EVALUATION ===
        # Build comprehensive game state for R1
        enhanced_game_state = {
            **game_state,  # Your existing game state
            'game_date': curr_ocr.get('date', 'Unknown'),
            'political_power': curr_pp,
            'screen_type': curr_screen,
            'civilian_factories': game_state['factories'].get('civilian', 0),
            'military_factories': game_state['factories'].get('military', 0),
        }

        # Record action and get strategic evaluation if triggered
        strategic_reward = self.strategic_reasoner.record_action(action, enhanced_game_state)

        if strategic_reward is not None:
            # R1 provided strategic evaluation
            reward += strategic_reward

            # Log significant strategic rewards
            if abs(strategic_reward) > 2.0:
                self.session_metrics['key_moments'].append({
                    'step': self.session_metrics['actions_taken'],
                    'description': f"DeepSeek-R1 strategic insight: {strategic_reward:+.2f}",
                    'reward': strategic_reward,
                    'strategy': self.strategic_reasoner.get_current_strategy()
                })

        return reward

    def describe_moment(self, action: Dict, reward: float) -> str:
        """Describe a key moment for logging"""
        if reward > 5:
            return f"Great success! {action['description']} led to major progress"
        elif reward > 2:
            return f"Good move: {action['description']}"
        elif reward < -2:
            return f"Setback after {action['description']}"
        else:
            return f"Explored {action['description']}"

    def detect_screen_type(self, ocr_data: Dict) -> str:
        """Detect current game screen from OCR data"""
        text_content = ' '.join(ocr_data.values()).lower()

        if 'production' in text_content and 'queue' in text_content:
            return 'production'
        elif 'construction' in text_content:
            return 'construction'
        elif 'research' in text_content:
            return 'research'
        elif 'focus' in text_content:
            return 'focus_tree'
        elif 'trade' in text_content:
            return 'trade'
        else:
            return 'main_map'

    def extract_number(self, text: str) -> int:
        """Extract number from text"""
        match = re.search(r'(\d+)', text)
        return int(match.group(1)) if match else 0

    def build_game_state(self, ocr_data: Dict) -> Dict:
        """Build game state from OCR data"""
        state = {
            'year': 1936,
            'month': 1,
            'factories': {'civilian': 0, 'military': 0},
            'territories_controlled': [],
            'current_screen': 'unknown'
        }

        # Extract date
        date_text = ocr_data.get('date', '')
        # Parse date (simplified)
        if '1936' in date_text:
            state['year'] = 1936
        elif '1937' in date_text:
            state['year'] = 1937
        elif '1938' in date_text:
            state['year'] = 1938
        elif '1939' in date_text:
            state['year'] = 1939

        # Extract factories
        factory_text = ocr_data.get('factories', '')
        numbers = re.findall(r'(\d+)', factory_text)
        if len(numbers) >= 2:
            state['factories']['civilian'] = int(numbers[0])
            state['factories']['military'] = int(numbers[1])

        return state

    def start_training(self):
        """Start or resume training"""
        self.is_training = True
        print("\n‚ñ∂Ô∏è Training active! AI is learning HOI4...")
        print("üß† Watch as it discovers game mechanics through pure exploration!")

        # ‚îÄ‚îÄ NEW: force one real menu switch so the agent gets +2 reward ‚îÄ‚îÄ
        bootstrap_action = {"type": "key",
                            "key": "f1",  # F1 = Politics screen
                            "description": "Press F1 (politics bootstrap)"}
        self.execute_action(bootstrap_action)
        # ----------------------------------------------------------------

    def pause_training(self):
        """Pause training"""
        self.is_training = False
        print("\n‚è∏Ô∏è Training paused")

    def save_progress(self):
        """Save current progress"""
        # Save AI checkpoint
        checkpoint_path = f"{CONFIG.checkpoint_dir}/ultimate_ai_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pth"
        self.ai.save_checkpoint(checkpoint_path)

        # Save session metrics
        metrics_path = f"checkpoints/session_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(metrics_path, 'w') as f:
            json.dump(self.session_metrics, f, indent=2)

        print(f"\nüíæ Progress saved!")

    def show_statistics(self):
        """Display current statistics"""
        stats = self.ai.get_statistics()

        print("\nüìä Ultimate AI Statistics")
        print("=" * 50)
        print(f"Total Steps: {stats['total_steps']:,}")
        print(f"Episode Steps: {stats['episode_steps']:,}")
        print(f"Total Reward: {stats['total_reward']:.2f}")
        print(f"Avg Intrinsic Reward: {stats['avg_intrinsic_reward']:.3f}")

        print(f"\nüß† Neural Episodic Control:")
        nec_stats = stats['nec_stats']
        print(f"  Memory Size: {nec_stats['memory_size']:,}/{50000:,}")
        print(f"  Lookups per Write: {nec_stats['lookups_per_write']:.1f}")

        print(f"\nüîç Curiosity (RND):")
        curiosity_stats = stats['curiosity_stats']
        print(f"  Mean Intrinsic: {curiosity_stats['mean_intrinsic_reward']:.3f}")
        print(f"  Max Intrinsic: {curiosity_stats['max_intrinsic_reward']:.3f}")

        print(f"\nüéØ Key Moments: {len(self.session_metrics['key_moments'])}")
        if self.session_metrics['key_moments']:
            latest = self.session_metrics['key_moments'][-1]
            print(f"  Latest: {latest['description']}")

    def update_status(self):
        """Update status line"""
        if self.is_training:
            runtime = (time.time() - self.session_start) / 60
            apm = self.session_metrics['actions_taken'] / runtime

            # Get current strategy from DeepSeek-R1
            current_strategy = self.strategic_reasoner.get_current_strategy()

            status = (
                f"\r‚ö° Actions: {self.session_metrics['actions_taken']} | "
                f"APM: {apm:.1f} | "
                f"Runtime: {runtime:.1f}m | "
                f"Key Moments: {len(self.session_metrics['key_moments'])} | "
                f"Phase: {current_strategy}"
            )
            print(status, end='', flush=True)

    def cleanup(self):
        """Clean up before exit"""

        self.shutdown_event.set()
        self.learn_thread.join(timeout=2)

        self.save_progress()

        # Final statistics
        print("\n\nüìä Final Session Summary")
        print("=" * 50)

        runtime = (time.time() - self.session_start) / 60
        print(f"Total Runtime: {runtime:.1f} minutes")
        print(f"Total Actions: {self.session_metrics['actions_taken']:,}")
        print(f"Actions per Minute: {self.session_metrics['actions_taken'] / runtime:.1f}")

        # === ADD THIS: DEEPSEEK-R1 STRATEGIC ANALYSIS ===
        strategic_summary = self.strategic_reasoner.get_strategic_summary()

        print(f"\nüß† DeepSeek-R1 Strategic Analysis:")
        print(f"  ‚Ä¢ Strategic evaluations performed: {strategic_summary['evaluations']}")
        print(f"  ‚Ä¢ Average strategic score: {strategic_summary['average_reward']:+.2f}")
        print(f"  ‚Ä¢ Current strategic phase: {strategic_summary['current_strategy']}")
        print(f"  ‚Ä¢ Strategic decisions made: {strategic_summary['total_decisions']}")

        if strategic_summary['recent_insights']:
            print(f"\nüí° Recent Strategic Insights from DeepSeek-R1:")
            for insight in strategic_summary['recent_insights'][-3:]:
                print(f"  ‚Ä¢ {insight.get('insight', 'No insight text')}")

        # Interesting discoveries
        if self.ai.curiosity.seen_screens:
            print(f"\nüó∫Ô∏è Screens Discovered: {len(self.ai.curiosity.seen_screens)}")
            for screen in sorted(self.ai.curiosity.seen_screens):
                print(f"  ‚Ä¢ {screen}")

        # Key moments
        if self.session_metrics['key_moments']:
            print(f"\n‚≠ê Top Key Moments:")
            sorted_moments = sorted(
                self.session_metrics['key_moments'],
                key=lambda x: x['reward'],
                reverse=True
            )[:5]

            for moment in sorted_moments:
                print(f"  ‚Ä¢ {moment['description']} (reward: {moment['reward']:.1f})")

        # Get successful strategies from persistent memory
        strategies = self.ai.persistent_memory.get_successful_strategies()
        if strategies:
            print(f"\nüí° Learned Strategies:")
            for strategy in strategies[:5]:
                print(f"  ‚Ä¢ {strategy['description'][:100]}...")

        print("\nüëã Thanks for training the Ultimate HOI4 AI!")


def integrate_with_main():
    """
    Function to integrate with your existing main.py
    Add this to your main.py file
    """

    def run_ultimate_mode(args):
        """Run the Ultimate AI mode"""
        print(f"\nüöÄ ULTIMATE AI MODE - DreamerV3 + RND + NEC")
        print(f"{'=' * 60}")
        print(f"This mode features:")
        print(f"  ‚úì World model that learns game dynamics")
        print(f"  ‚úì Curiosity-driven exploration (RND)")
        print(f"  ‚úì Fast learning from experience (NEC)")
        print(f"  ‚úì Persistent memory across games")
        print(f"  ‚úì Pure self-play learning")

        # Create trainer instance
        ultimate_trainer = UltimateTrainer()

        # Run training loop
        ultimate_trainer.run()

    return run_ultimate_mode


# Standalone entry point
if __name__ == "__main__":
    trainer_instance = UltimateTrainer()
    trainer_instance.run()